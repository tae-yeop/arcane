{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Sxela/ArcaneGAN/releases 에서 jit 다운받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define functions\n",
    "#@markdown Select model version and run.\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "import torch, PIL\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "mtcnn = MTCNN(image_size=256, margin=80)\n",
    "\n",
    "# simplest ye olde trustworthy MTCNN for face detection with landmarks\n",
    "def detect(img):\n",
    " \n",
    "        # Detect faces\n",
    "        batch_boxes, batch_probs, batch_points = mtcnn.detect(img, landmarks=True)\n",
    "        # Select faces\n",
    "        if not mtcnn.keep_all:\n",
    "            batch_boxes, batch_probs, batch_points = mtcnn.select_boxes(\n",
    "                batch_boxes, batch_probs, batch_points, img, method=mtcnn.selection_method\n",
    "            )\n",
    " \n",
    "        return batch_boxes, batch_points\n",
    "\n",
    "# my version of isOdd, should make a separate repo for it :D\n",
    "def makeEven(_x):\n",
    "  return _x if (_x % 2 == 0) else _x+1\n",
    "\n",
    "# the actual scaler function\n",
    "def scale(boxes, _img, max_res=1_500_000, target_face=256, fixed_ratio=0, max_upscale=2, VERBOSE=False):\n",
    " \n",
    "    x, y = _img.size\n",
    " \n",
    "    ratio = 2 #initial ratio\n",
    " \n",
    "    #scale to desired face size\n",
    "    if (boxes is not None):\n",
    "      if len(boxes)>0:\n",
    "        ratio = target_face/max(boxes[0][2:]-boxes[0][:2]); \n",
    "        ratio = min(ratio, max_upscale)\n",
    "        if VERBOSE: print('up by', ratio)\n",
    "\n",
    "    if fixed_ratio>0:\n",
    "      if VERBOSE: print('fixed ratio')\n",
    "      ratio = fixed_ratio\n",
    " \n",
    "    x*=ratio\n",
    "    y*=ratio\n",
    " \n",
    "    #downscale to fit into max res \n",
    "    res = x*y\n",
    "    if res > max_res:\n",
    "      ratio = pow(res/max_res,1/2); \n",
    "      if VERBOSE: print(ratio)\n",
    "      x=int(x/ratio)\n",
    "      y=int(y/ratio)\n",
    " \n",
    "    #make dimensions even, because usually NNs fail on uneven dimensions due skip connection size mismatch\n",
    "    x = makeEven(int(x))\n",
    "    y = makeEven(int(y))\n",
    "    \n",
    "    size = (x, y)\n",
    "\n",
    "    return _img.resize(size)\n",
    "\n",
    "\"\"\" \n",
    "    A useful scaler algorithm, based on face detection.\n",
    "    Takes PIL.Image, returns a uniformly scaled PIL.Image\n",
    "    boxes: a list of detected bboxes\n",
    "    _img: PIL.Image\n",
    "    max_res: maximum pixel area to fit into. Use to stay below the VRAM limits of your GPU.\n",
    "    target_face: desired face size. Upscale or downscale the whole image to fit the detected face into that dimension.\n",
    "    fixed_ratio: fixed scale. Ignores the face size, but doesn't ignore the max_res limit.\n",
    "    max_upscale: maximum upscale ratio. Prevents from scaling images with tiny faces to a blurry mess.\n",
    "\"\"\"\n",
    "\n",
    "def scale_by_face_size(_img, max_res=1_500_000, target_face=256, fix_ratio=0, max_upscale=2, VERBOSE=False):\n",
    "    boxes = None\n",
    "    boxes, _ = detect(_img)\n",
    "    if VERBOSE: print('boxes',boxes)\n",
    "    img_resized = scale(boxes, _img, max_res, target_face, fix_ratio, max_upscale, VERBOSE)\n",
    "    return img_resized\n",
    "\n",
    "\n",
    "size = 256\n",
    "\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_stds = torch.tensor(stds).cuda().half()[:,None,None]\n",
    "t_means = torch.tensor(means).cuda().half()[:,None,None]\n",
    "\n",
    "def makeEven(_x):\n",
    "  return int(_x) if (_x % 2 == 0) else int(_x+1)\n",
    "\n",
    "img_transforms = transforms.Compose([                        \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means,stds)])\n",
    " \n",
    "def tensor2im(var):\n",
    "     return var.mul(t_stds).add(t_means).mul(255.).clamp(0,255).permute(1,2,0)\n",
    "\n",
    "def proc_pil_img(input_image, model):\n",
    "    transformed_image = img_transforms(input_image)[None,...].cuda().half()\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        result_image = model(transformed_image)[0]; print(result_image.shape)\n",
    "        output_image = tensor2im(result_image)\n",
    "        output_image = output_image.detach().cpu().numpy().astype('uint8')\n",
    "        output_image = PIL.Image.fromarray(output_image)\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "\n",
    "version = '0.4' #@param ['0.1','0.2','0.3','0.4']\n",
    "\n",
    "model_path = f'/home/aiteam/tykim/cubox/arcanegan/ArcaneGANv0.4.jit'\n",
    "in_dir = '/home/aiteam/tykim/cubox/arcanegan/src'\n",
    "out_dir = f\"/home/aiteam/tykim/cubox/arcanegan/dest\"\n",
    "\n",
    "model = torch.jit.load(model_path).eval().cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output \n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def reset(p):\n",
    "  with output_reset:\n",
    "    clear_output()\n",
    "  clear_output()\n",
    "  process()\n",
    " \n",
    "button_reset = widgets.Button(description=\"Upload\")\n",
    "output_reset = widgets.Output()\n",
    "button_reset.on_click(reset)\n",
    "\n",
    "def fit(img,maxsize=512):\n",
    "  maxdim = max(*img.size)\n",
    "  if maxdim>maxsize:\n",
    "    ratio = maxsize/maxdim\n",
    "    x,y = img.size\n",
    "    size = (int(x*ratio),int(y*ratio)) \n",
    "    img = img.resize(size)\n",
    "  return img\n",
    " \n",
    "def show_img(f, size=1024):\n",
    "  display(fit(PIL.Image.open(f),size))\n",
    "\n",
    "def process(src_path, dest_path):\n",
    "  in_files = sorted(glob(f'{src_path}/*'))\n",
    "  for img in tqdm(in_files):\n",
    "    out = f\"{dest_path}/{img.split('/')[-1].split('.')[0]}.jpg\"\n",
    "    im = PIL.Image.open(img).resize((512, 512)).convert(\"RGB\") \n",
    "    im = scale_by_face_size(im, target_face=300, max_res=1_500_000, max_upscale=1)\n",
    "    res = proc_pil_img(im, model)\n",
    "    res.save(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add9d46379614d35a46edd6732c3cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "process(in_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
